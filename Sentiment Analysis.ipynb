{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(seed=132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import the data from '1.txt', '2.txt', '3.txt' and will store them as dataframes in df1, df2, test_df respectively and will name the column as 'Text'. Also we will extract the sentiments(0/1) from the 'Text' columns and will store them in 'Prediction' column for both dataframes df1 and df2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file from first file\n",
    "df1 = pd.read_csv(r'C:\\Users\\Sony\\Desktop\\riverus\\1.txt', sep='.   ', header=None)\n",
    "df1.columns = ['Text']\n",
    "\n",
    "sentiment = pd.Series([])\n",
    "\n",
    "i=0;\n",
    "for text in df1['Text']:\n",
    "    sentiment[i] = text[-1:]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Prediction'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 952 entries, 0 to 951\n",
      "Data columns (total 2 columns):\n",
      "Text          952 non-null object\n",
      "Prediction    952 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(r'C:\\Users\\Sony\\Desktop\\riverus\\2.txt', sep='   .', header=None)\n",
    "df2.columns = ['Text']\n",
    "\n",
    "sentiment2 = pd.Series([])\n",
    "\n",
    "j=0;\n",
    "for text in df2['Text']:\n",
    "    sentiment2[j] = text[:1]\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Prediction'] = sentiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7086 entries, 0 to 7085\n",
      "Data columns (total 2 columns):\n",
      "Text          7086 non-null object\n",
      "Prediction    7086 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 110.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Text Prediction\n",
      "count                      7086       7086\n",
      "unique                     1411          2\n",
      "top     1\\tI love Harry Potter.          1\n",
      "freq                        167       3995\n"
     ]
    }
   ],
   "source": [
    "print (df2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(r'C:\\Users\\Sony\\Desktop\\riverus\\3.txt', sep='  .')\n",
    "test_df.columns = ['Text']\n",
    "\n",
    "test_df['Text'] = test_df['Text'].str.lower()\n",
    "test_df['Text']= test_df['Text'].map(lambda x: re.sub('[,\\.!?\"\\t\\d+]', '', x))\n",
    "\n",
    "test_df = test_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 919 entries, 0 to 931\n",
      "Data columns (total 1 columns):\n",
      "Text    919 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 14.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(r'C:\\Users\\Sony\\Desktop\\riverus\\test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge both the training dataframes df1 and df2 into one single dataframe named as df on which we will train our model. After merging we will also perform some text preprocessing like converting all alphabates to lower and including only alphabates in our 'Text' column. Also there are many duplicates rows in the dataframe, so we will drop all the duplicate rows and will shuffle all the rows to build a unbiased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1, df2, how='outer')\n",
    "df['Prediction'] = df['Prediction'].astype(int)\n",
    "\n",
    "\n",
    "df['Text'] = df['Text'].str.lower()\n",
    "df['Text']= df['Text'].map(lambda x: re.sub('[^a-zA-z\\s]', '', x))\n",
    "\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2176 entries, 4996 to 355\n",
      "Data columns (total 2 columns):\n",
      "Text          2176 non-null object\n",
      "Prediction    2176 non-null int32\n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 42.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Sony\\Desktop\\riverus\\train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1136\n",
      "0    1040\n",
      "Name: Prediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = df.Prediction.value_counts()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Counts')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = pd.DataFrame(x)\n",
    "x['Rating'] = x.index\n",
    "sns.set_style(\"whitegrid\", {\"axes.facecolor\": \".9\"})\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.barplot(y='Prediction', x='Rating', data=x)\n",
    "plt.xlabel('Sentiments')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import some Usefull libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sony\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will see the frequency of different words in our dataset, and we will notice that there are a large frequency of stopwords available in our dataset, so this is how this part of code help us in deciding whether we need to remove these stopwords or not for our further process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potter</th>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harry</th>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinci</th>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brokeback</th>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mountain</th>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impossible</th>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mission</th>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nc</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nature</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smart</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>narrative</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>documentary</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donkey</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>murdered</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clichs</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouth</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soon</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dramatic</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dream</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>due</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moments</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moment</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speak</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoilers</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mickey</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mi</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clearly</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meaning</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easily</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matter</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             frequency\n",
       "the               1488\n",
       "and                796\n",
       "is                 605\n",
       "of                 542\n",
       "to                 472\n",
       "was                432\n",
       "it                 396\n",
       "potter             332\n",
       "harry              329\n",
       "vinci              323\n",
       "this               322\n",
       "da                 320\n",
       "brokeback          315\n",
       "code               314\n",
       "mountain           313\n",
       "movie              311\n",
       "that               290\n",
       "in                 284\n",
       "impossible         274\n",
       "mission            270\n",
       "but                219\n",
       "love               218\n",
       "awesome            202\n",
       "for                179\n",
       "film               166\n",
       "so                 161\n",
       "as                 157\n",
       "you                146\n",
       "like               145\n",
       "with               136\n",
       "...                ...\n",
       "nc                   3\n",
       "nature               3\n",
       "smart                3\n",
       "narrative            3\n",
       "documentary          3\n",
       "donkey               3\n",
       "murdered             3\n",
       "clichs               3\n",
       "mouth                3\n",
       "soon                 3\n",
       "dramatic             3\n",
       "dream                3\n",
       "due                  3\n",
       "moments              3\n",
       "student              3\n",
       "moment               3\n",
       "space                3\n",
       "speak                3\n",
       "spoilers             3\n",
       "star                 3\n",
       "stars                3\n",
       "mickey               3\n",
       "mi                   3\n",
       "clearly              3\n",
       "stop                 3\n",
       "meaning              3\n",
       "early                3\n",
       "easily               3\n",
       "matter               3\n",
       "ability              3\n",
       "\n",
       "[994 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = CountVectorizer(ngram_range=(1,1), analyzer='word', min_df=0.001)\n",
    "sparse_matrix = word_vectorizer.fit_transform(df['Text'])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "freq = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "freq.sort_values('frequency', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here after tokenisation will perform stemming on our 'Text' column and will join those tokens again and will store that in a new column named 'tokezines_sents'. We will also perform train test split our training data and will transform our X_train_df and X_test_df to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokezines_sents'] = df.apply(lambda x: nltk.word_tokenize(x['Text']),axis=1)\n",
    "stemmer = SnowballStemmer('english')\n",
    "df['tokezines_sents'] = df['tokezines_sents'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df['tokezines_sents'] = df['tokezines_sents'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X = df['tokezines_sents']\n",
    "y=df['Prediction']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=53)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = vectorizer.fit_transform(X_train)\n",
    "X_test_df = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build our 1st model using LogisticRegression as a classifier and will also perform gridsearch to choose best parameters to get a better model. And after that we will fit our model on X_test and will calculate the accuracy and classification matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "\n",
    "params = {'penalty':['l1','l2'],\n",
    "         'C':[0.01,0.1,1,10,100],\n",
    "         'class_weight':['balanced',None]}\n",
    "\n",
    "model = GridSearchCV(clf1, params, cv=10)\n",
    "\n",
    "model.fit(X_train_df, y_train)\n",
    "y_pred = model.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'class_weight': None, 'penalty': 'l2'}\n",
      "0.8698315467075038\n",
      "[[253  44]\n",
      " [ 41 315]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86       297\n",
      "           1       0.88      0.88      0.88       356\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       653\n",
      "   macro avg       0.87      0.87      0.87       653\n",
      "weighted avg       0.87      0.87      0.87       653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.best_params_)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that our second model will be build using MultinomialNB as a classifier and best alpha is choosed using gridsearch, and accuracy and classification matrix is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = MultinomialNB()\n",
    "\n",
    "params2 = {'alpha':[0.01,0.1,1,10,100]}\n",
    "\n",
    "model2 = GridSearchCV(clf2, params2, cv=10)\n",
    "\n",
    "model2.fit(X_train_df, y_train)\n",
    "y_pred2 = model2.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1}\n",
      "0.889739663093415\n",
      "[[261  36]\n",
      " [ 36 320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       297\n",
      "           1       0.90      0.90      0.90       356\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       653\n",
      "   macro avg       0.89      0.89      0.89       653\n",
      "weighted avg       0.89      0.89      0.89       653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model2.best_params_)\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see out of the above two model, model2 is performing better and giving us a higher accuracy. So we will perform some preprocessing on our test_df and will predict the sentiments using model2 which is using Naive Bayes's MultinomialNB classifier. And before predicting we will also train the same model for complete training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tokezines_sents'] = test_df.apply(lambda x: nltk.word_tokenize(x['Text']),axis=1)\n",
    "test_df['tokezines_sents'] = test_df['tokezines_sents'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "test_df['tokezines_sents'] = test_df['tokezines_sents'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_final = test_df['tokezines_sents']\n",
    "\n",
    "X_dy = vectorizer.fit_transform(X)\n",
    "X_final_test = vectorizer.transform(X_final)\n",
    "\n",
    "model2.fit(X_dy, y)\n",
    "final_prediction = model2.predict(X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Prediction'] = final_prediction.tolist()\n",
    "test_df = test_df.drop('tokezines_sents', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i exchanged the sony ericson za for this and i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oh and i forgot to also mention the weird colo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>verizon tech support walked my through a few p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>better than you'd expect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a great little item</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Prediction\n",
       "0  i exchanged the sony ericson za for this and i...           1\n",
       "1  oh and i forgot to also mention the weird colo...           1\n",
       "2  verizon tech support walked my through a few p...           0\n",
       "3                           better than you'd expect           0\n",
       "4                        this is a great little item           1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(r'C:\\Users\\Sony\\Desktop\\riverus\\Prediction_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
